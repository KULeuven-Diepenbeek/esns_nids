{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ebedc9",
   "metadata": {},
   "source": [
    "## Flow-based features\n",
    "\n",
    "While using one packet at a time to detect intrusions is very simple and straightforward, one can argue that relevant context is lost: The model does not consider the position of the packet in a flow. Therefor, most RTF-based machine learning models in literature consider the packet flows rather than individual packets. Recall that a network flow is identified by its 5-tuple: Source address, destination address, source port, destination port and protocol. The main idea is to sort flows based on their 5-tuple (flow ID), and then extract features from the resulting flows. In the literature, this is usually done as follows:\n",
    "\n",
    "1. Iterate over the dataset in chronological order\n",
    "    1. For each packet, extract the flow ID (if applicable)\n",
    "    2. Store the entire packet (or a truncated version) in a data structure containing all packets with that flow ID\n",
    "2. Decide on the dimensions of your input features\n",
    "3. Iterate over each flow\n",
    "    1. Extract the input features\n",
    "    2. Store the extracted features\n",
    "\n",
    "These extracted, stored features can then be used for training and or model evaluation. Deciding on the input feature dimensions not has become slightly more complicated however, when compared to using individual packets. Mainly, there are two decisions that need to be made:\n",
    "\n",
    "1. How many packets do you include for one sample;\n",
    "2. Do you only consider the beginning of the flow, or the entire flow.\n",
    "\n",
    "As an example, the model we will use in this exercise was trained on samples of 4x64 bytes, created by taking the first 64 bytes of 4 subsequent packets in a flow. All packets in such a flow were considered.\n",
    "\n",
    "### In hardware\n",
    "\n",
    "The above approach does not work in hardware, as it requires having access to the entire dataset at once, and being able to store that entire dataset at once while processing its data. When performing network intrusion detection in hardware, there is only a limited amount of storage and it is impossible to predict what future packets will arrive. Therefor, another approach is necessary. One solution is to sort incoming packets into buckets according to their flow ID, and to empty those buckets whenever there needs to be made space for new incoming packets. Emptying a bucket then amounts to creating an input sample for the detection model. (Image: *n* individual buckets, packets are stored according to their flow ID FID).\n",
    "\n",
    "<img src=\"./images/32_sorting.gif\" width=\"600\" height=\"150\" />\n",
    "\n",
    "### In softeware: exercise\n",
    "In software, both approaches are possible: Either sorting the entire dataset beforehand, or filling and emptying buckets based on the incoming traffic. In this exercise, we will sort the entire dataset dataset for extracting features and conducting inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f8e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from lib.dataset import NIDSDataset\n",
    "\n",
    "\n",
    "# Initialize the dataset\n",
    "dset = NIDSDataset(\n",
    "    packets_file=\"./data/dataset_packets_v2.npy\", \n",
    "    labels_file=\"./data/dataset_labels_v1.npy\")\n",
    "\n",
    "# The flow_dict contains a list of packets for each flow ID:\n",
    "# flow_dict = {\n",
    "#    \"id_1\": [..., ...],\n",
    "#    \"id_2\": [..., ...],\n",
    "#    ...\n",
    "# }\n",
    "flow_dict = {}\n",
    "\n",
    "# Also include the label in the flow ID to account for the possibility that different packets from the same flow \n",
    "# could be malicious.\n",
    "# Iterate over the dataset, sort all valid input features to dictionary\n",
    "\n",
    "for packet in dset:\n",
    "    label = packet.get_label()\n",
    "    \n",
    "    # Own code here\n",
    "    \n",
    "    for word in packet:\n",
    "        # Own code here\n",
    "        pass\n",
    "    \n",
    "    \n",
    "print(\"We extracted {} flow IDs.\".format(len(flow_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718816b",
   "metadata": {},
   "source": [
    "Once the packets have been sorted, you can use the dictionary to extract your actual features.\n",
    "Tip: Initialize your input samples using *np.zeros(...)*, so that input samples that only have one, two or three instead of four packets available can account for the missing data. Similarly, in hardware the missing bytes would have to be zero-padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1be274",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_buffer = []\n",
    "label_buffer = []\n",
    "\n",
    "for flow_id, packet_list in flow_dict.items():\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "print(\"We prepared {} input samples\".format(len(input_buffer)))\n",
    "\n",
    "# The items in the input_buffer should be 320-sized numpy arrays\n",
    "# These are then transform to tensors\n",
    "input_tensors = []\n",
    "\n",
    "for input_sample in input_buffer:\n",
    "    # Turn the Numpy array into a PyTorch tensor\n",
    "    input_tensor = torch.from_numpy(input_sample)\n",
    "    # Change input dimensionality\n",
    "    input_tensor = input_tensor.view(1, 1, 4 * 64)\n",
    "    \n",
    "    input_tensors.append(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583b0df",
   "metadata": {},
   "source": [
    "Now we repeat the process of loading the CNN, performance inference and interpreting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.nn_model import ExampleCNN1D4x64\n",
    "\n",
    "model = ExampleCNN1D4x64(13)\n",
    "\n",
    "# Load the trained parameters\n",
    "model.load_state_dict(torch.load(\"./data/cnn1d4x64.model\", map_location=torch.device('cpu')))\n",
    "\n",
    "# Set the Batch Normalization layers for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754cd055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.nn_model import label_mapping\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for input_tensor in input_tensors:\n",
    "    output_tensor = model(input_tensor.float())\n",
    "    \n",
    "    _, predicted = torch.max(output_tensor, 1)\n",
    "    predictions.append(predicted)\n",
    "\n",
    "predictions = torch.stack(predictions, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d16119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Transform the indices to their corresponding class label:\n",
    "labelled_predictions = []\n",
    "for prediction in predictions:\n",
    "    labelled_predictions.append(label_mapping[prediction[0]])\n",
    "\n",
    "# Choose output figure size\n",
    "_, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(label_buffer, labelled_predictions, labels=label_mapping)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=label_mapping)\n",
    "disp.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(label_buffer, labelled_predictions, \n",
    "                            labels=label_mapping, \n",
    "                            target_names=label_mapping,\n",
    "                            digits=4,\n",
    "                            zero_division=0\n",
    "                           ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d197f",
   "metadata": {},
   "source": [
    "We expect the following results:\n",
    "- BENIGN F1 = 0.9421\n",
    "- Bot F1 = 0.8235\n",
    "- PortScan = 0.4684\n",
    "- DDoS = 0.5741\n",
    "- F1_weighted_avg = 0.7353\n",
    "\n",
    "Now, clearly the generated samples were significantly more poorly classified using this second model, when compared to the model with simple input features. However, it should be noted that this is only a very limited representation of the the model performance, mainly due to the limited dataset size.\n",
    "\n",
    "For a more fair comparison, we will briefly provide the full performance details for both models here:\n",
    "Both models were trained on the entire CICIDS2017 dataset, specifically for the purpose of this workshop. We first sorted the entire dataset in flows, before extracting labelled samples. The extracted data was split in a training, validation and test set, consisting of respectively 75%, 15% and 10% of the dataset. We trained both models for 25 epochs at a learning rate of 0.0001 and a minibatch-size of 64. For the 1x64-model, each packet in a labelled flow constitutes an indivual sample, while the 4x64-model uses groups of 4 subsequent packets. Missing packets or packet bytes were zero-padded. For this configuration, we obtained the following results:\n",
    "\n",
    "For *ExampleCNN1D1x64*:\n",
    "```\n",
    "                  precision    recall       fpr  f1-score       mcc   support\n",
    "           BENIGN     99.99     99.82      0.09     99.91     98.81   1036764\n",
    "              Bot     99.18     77.74      0.00     87.16     87.80       310\n",
    "         PortScan    100.00     77.01      0.00     87.01     87.76        87\n",
    "             DDoS     99.81    100.00      0.00     99.90     99.90     10973\n",
    "       Web Attack     95.45     99.46      0.00     97.42     97.44       929\n",
    "     Infiltration    100.00    100.00      0.00    100.00    100.00       598\n",
    "    DoS GoldenEye     94.03     96.78      0.01     95.38     95.38      2605\n",
    "         DoS Hulk     97.20     99.83      0.18     98.50     98.41     64536\n",
    " DoS Slowhttptest     92.57     93.21      0.01     92.89     92.88      1163\n",
    "    DoS slowloris     97.62     96.03      0.00     96.82     96.82      1412\n",
    "       Heartbleed    100.00    100.00      0.00    100.00    100.00       417\n",
    "      FTP-Patator     99.53    100.00      0.00     99.77     99.77      2568\n",
    "      SSH-Patator    100.00     99.96      0.00     99.98     99.98      2357\n",
    "         accuracy                                             99.80   1124719\n",
    "        macro avg     98.11     95.37      0.02     96.52     96.54   1124719\n",
    "        micro avg     99.80     99.80      0.02     99.80     99.78   1124719\n",
    "     weighted avg     99.80     99.80      0.09     99.80     98.78   1124719\n",
    "```\n",
    "\n",
    "For *ExampleCNN1D4x64*:\n",
    "\n",
    "```\n",
    "                  precision    recall       fpr  f1-score       mcc   support\n",
    "           BENIGN     99.99     99.82      0.09     99.91     98.84   1036764\n",
    "              Bot     96.93     81.61      0.00     88.62     88.94       310\n",
    "         PortScan     97.40     86.21      0.00     91.46     91.63        87\n",
    "             DDoS     99.86    100.00      0.00     99.93     99.93     10973\n",
    "       Web Attack     98.94    100.00      0.00     99.46     99.47       929\n",
    "     Infiltration    100.00    100.00      0.00    100.00    100.00       598\n",
    "    DoS GoldenEye     97.39     95.97      0.01     96.67     96.67      2605\n",
    "         DoS Hulk     97.20     99.99      0.18     98.57     98.49     64536\n",
    " DoS Slowhttptest     96.86     95.61      0.00     96.24     96.23      1163\n",
    "    DoS slowloris     98.57     97.31      0.00     97.93     97.93      1412\n",
    "       Heartbleed    100.00    100.00      0.00    100.00    100.00       417\n",
    "      FTP-Patator     99.69     99.81      0.00     99.75     99.75      2568\n",
    "      SSH-Patator    100.00     99.96      0.00     99.98     99.98      2357\n",
    "         accuracy                                             99.81   1124719\n",
    "        macro avg     98.68     96.64      0.02     97.58     97.53   1124719\n",
    "        micro avg     99.81     99.81      0.02     99.81     99.80   1124719\n",
    "     weighted avg     99.82     99.81      0.09     99.81     98.82   1124719\n",
    "```\n",
    "Note: *mcc* is the Matthews Correlation Coefficient, *fpr* is the False Positive Rate.\n",
    "\n",
    "Both models achieve good results, with the *ExampleCNN1D4x64* model actually slightly surpassing its counterpart in both weighted and macro f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a0a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
