{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c025865",
   "metadata": {},
   "source": [
    "## Flow-based features\n",
    "\n",
    "While using one packet at a time to detect intrusions is very simple and straightforward, one can argue that relevant context is lost: The model does not consider the position of the packet in a flow. Therefor, most RTF-based machine learning models in literature consider the packet flows rather than individual packets. Recall that a network flow is identified by its 5-tuple: Source address, destination address, source port, destination port and protocol. The main idea is to sort flows based on their 5-tuple (flow ID), and then extract features from the resulting flows. In the literature, this is usually done as follows:\n",
    "\n",
    "1. Iterate over the dataset in chronological order\n",
    "    1. For each packet, extract the flow ID (if applicable)\n",
    "    2. Store the entire packet (or a truncated version) in a data structure containing all packets with that flow ID\n",
    "2. Decide on the dimensions of your input features\n",
    "3. Iterate over each flow\n",
    "    1. Extract the input features\n",
    "    2. Store the extracted features\n",
    "\n",
    "These extracted, stored features can then be used for training and or model evaluation. Deciding on the input feature dimensions not has become slightly more complicated however, when compared to using individual packets. Mainly, there are two decisions that need to be made:\n",
    "\n",
    "1. How many packets do you include for one sample;\n",
    "2. Do you only consider the beginning of the flow, or the entire flow.\n",
    "\n",
    "As an example, the model we will use in this exercise was trained on samples of 4x64 bytes, created by taking the first 64 bytes of 4 subsequent packets in a flow. All packets in such a flow were considered.\n",
    "\n",
    "### In hardware\n",
    "\n",
    "The above approach does not work in hardware, as it requires having access to the entire dataset at once, and being able to store that entire dataset at once while processing its data. When performing network intrusion detection in hardware, there is only a limited amount of storage and it is impossible to predict what future packets will arrive. Therefor, another approach is necessary. One solution is to sort incoming packets into buckets according to their flow ID, and to empty those buckets whenever there needs to be made space for new incoming packets. Emptying a bucket then amounts to creating an input sample for the detection model. (Image: *n* individual buckets, packets are stored according to their flow ID FID).\n",
    "\n",
    "<img src=\"./images/32_sorting.gif\" width=\"600\" height=\"150\" />\n",
    "\n",
    "### In softeware: exercise\n",
    "In software, both approaches are possible: Either sorting the entire dataset beforehand, or filling and emptying buckets based on the incoming traffic. In this exercise, we will sort the entire dataset dataset for extracting features and conducting inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5093ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We extracted 0 flow IDs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from lib.dataset import NIDSDataset\n",
    "\n",
    "\n",
    "# Initialize the dataset\n",
    "dset = NIDSDataset(\n",
    "    packets_file=\"./data/dataset_packets_v2.npy\", \n",
    "    labels_file=\"./data/dataset_labels_v1.npy\")\n",
    "\n",
    "# The flow_dict contains a list of packets for each flow ID:\n",
    "# flow_dict = {\n",
    "#    \"id_1\": [..., ...],\n",
    "#    \"id_2\": [..., ...],\n",
    "#    ...\n",
    "# }\n",
    "flow_dict = {}\n",
    "\n",
    "# Also include the label in the flow ID to account for the possibility that different packets from the same flow \n",
    "# could be malicious.\n",
    "# Iterate over the dataset, sort all valid input features to dictionary\n",
    "\n",
    "for packet in dset:\n",
    "    label = packet.get_label()\n",
    "    \n",
    "    # Own code here\n",
    "    \n",
    "    for word in packet:\n",
    "        # Own code here\n",
    "        pass\n",
    "    \n",
    "    \n",
    "print(\"We extracted {} flow IDs.\".format(len(flow_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a12b5c",
   "metadata": {},
   "source": [
    "Once the packets have been sorted, you can use the dictionary to extract your actual features.\n",
    "Tip: Initialize your input samples using *np.zeros(...)*, so that input samples that only have one, two or three instead of four packets available can account for the missing data. Similarly, in hardware the missing bytes would have to be zero-padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_buffer = []\n",
    "label_buffer = []\n",
    "\n",
    "for flow_id, packet_list in flow_dict.items():\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "print(\"We prepared {} input samples\".format(len(input_buffer)))\n",
    "\n",
    "# The items in the input_buffer should be 320-sized numpy arrays\n",
    "# These are then transform to tensors\n",
    "input_tensors = []\n",
    "\n",
    "for input_sample in input_buffer:\n",
    "    # Turn the Numpy array into a PyTorch tensor\n",
    "    input_tensor = torch.from_numpy(input_sample)\n",
    "    # Change input dimensionality\n",
    "    input_tensor = input_tensor.view(1, 1, 4 * 64)\n",
    "    \n",
    "    input_tensors.append(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df6895",
   "metadata": {},
   "source": [
    "Now we repeat the process of loading the CNN, performance inference and interpreting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3aeb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleCNN1D4x64(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv1d(64, 96, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(96, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool4): MaxPool1d(kernel_size=16, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (classification): Linear(in_features=96, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.nn_model import ExampleCNN1D4x64\n",
    "\n",
    "model = ExampleCNN1D4x64(13)\n",
    "\n",
    "# Load the trained parameters\n",
    "model.load_state_dict(torch.load(\"./data/cnn1d4x64.model\"))\n",
    "\n",
    "# Set the Batch Normalization layers for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.nn_model import label_mapping\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for input_tensor in input_tensors:\n",
    "    output_tensor = model(input_tensor.float())\n",
    "    \n",
    "    _, predicted = torch.max(output_tensor, 1)\n",
    "    predictions.append(predicted)\n",
    "\n",
    "predictions = torch.stack(predictions, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993fce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Transform the indices to their corresponding class label:\n",
    "labelled_predictions = []\n",
    "for prediction in predictions:\n",
    "    labelled_predictions.append(label_mapping[prediction[0]])\n",
    "\n",
    "# Choose output figure size\n",
    "_, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(label_buffer, labelled_predictions, labels=label_mapping)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=label_mapping)\n",
    "disp.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(label_buffer, labelled_predictions, \n",
    "                            labels=label_mapping, \n",
    "                            target_names=label_mapping,\n",
    "                            digits=4,\n",
    "                            zero_division=0\n",
    "                           ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0284955",
   "metadata": {},
   "source": [
    "We expect the following results:\n",
    "- BENIGN F1 = 0.9421\n",
    "- Bot F1 = 0.8235\n",
    "- PortScan = 0.4684\n",
    "- DDoS = 0.5741\n",
    "- F1_weighted_avg = 0.7353\n",
    "\n",
    "Now, clearly the generated samples were significantly more poorly classified using this second model, when compared to the model with simple input features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6aa288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
