{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIDS based on a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning for network intrusion detecion\n",
    "As opposed to traditional approaches, such as using regular expressions, Machine Learning (ML)-based network intrusion detection provides a new approach to detecting attacks. Rather than having a security expert hardcoding security rules and using those to stop attacks, ML offers an attractive alternative: Run some algorithm on a dataset, let it decide for itself what traffic is anomalous or malicious, and use that functionality to protect a network. No more need for manual rule definitions, or even expert network knowledge to some extent. However, this new approach introduces new problems that need to be solved:\n",
    "\n",
    "- High-quality datasets are used for training and evaluating model;\n",
    "- ML introduces new challengs with regards to false positives and negatives;\n",
    "- How to keep a model up-to-date with changing traffic environments;\n",
    "- ML computations are compute-and/or memory-intensive, which complicates their scalability for high throughput data streams;\n",
    "- ...\n",
    "\n",
    "The advent of Deep Learning (DL) offers new opportunities with more complex and powerful learning models, but does not solve all problems. Mostly, DL provides neural networks that better model a dataset, yielding detection results with fewer errors. In this workshop, we will explore a small part of this field, trying to detect attacks in the provided dataset. We will introduce some models while trying to keep track of the hardware perspective. \n",
    "\n",
    "For this purpose, we will the [PyTorch](https://pytorch.org/) framework. This exercise will introduce you some basics that are necessary in this exercise.\n",
    "\n",
    "### Neural networks in PyTorch\n",
    "\n",
    "In PyTorch, a neural network is defined as a class that inherits from the torch.nn.Module class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51766/1782131619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mExampleNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ExampleNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(ExampleNN, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.layer1 = nn.Linear(in_features= 64, out_features=100, bias=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.layer2 = nn.Linear(in_features= 100, out_features=100, bias=True)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.layer3 = nn.Linear(in_features= 100, out_features=self.num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above network, *ExampleNN*, is a dummy example with an input layer with 64 neurons, two hidden layers with 100 neurons and an output layer with *num_classes* outputs. The two hidden layers are followed by a Rectified Linear Unit (ReLU) that serves as the nonlinear activation fuction.\n",
    "\n",
    "Interfacing with this neural network is as simple as generating an input sample and providing it at the input of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0998, -0.0186, -0.3195, -0.5058,  0.4509,  0.3454, -0.6073,  0.4325,\n",
       "          0.5345,  0.6547,  2.3460, -0.2233,  0.7465,  0.0529, -1.2621, -0.8823,\n",
       "          0.7075, -0.1546,  0.5815,  0.3936, -0.9759,  0.7646,  1.2548, -1.8848,\n",
       "          0.3669,  1.1147,  1.8127,  1.9427,  1.2748, -0.9963,  0.4521, -1.5941,\n",
       "          0.8702,  0.3257, -0.3275,  0.3519, -0.0509,  0.3846,  0.0783, -0.7513,\n",
       "         -0.4391, -1.5458, -0.2681, -0.5479,  0.2987,  0.4221,  0.1181, -1.1920,\n",
       "          0.8267,  0.6238,  0.3716,  1.0232, -0.9450, -1.8257, -0.0172,  0.4054,\n",
       "          0.3393,  0.5125, -0.9881, -1.0161,  1.5061,  0.2318, -2.5841,  0.6397]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a random 64-sized input tensor:\n",
    "x = torch.randn(1, 64)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0246,  0.0573,  0.0803,  0.1455, -0.1184, -0.3049,  0.1403,  0.1088,\n",
       "         -0.3098, -0.2272,  0.0676, -0.3516, -0.0196]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a neural network with 13 output units:\n",
    "net = ExampleNN(13)\n",
    "\n",
    "# Run the input tensor through the network to receive 3 outputs:\n",
    "y = net(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting output should then be used to make some prediction, or to evaluate the performance of the learning model. Of course, the model should first be trained on a dataset. However, due to the limited time for this workshop, we will only consider the extraction of input features as well as the evaluation of output values for models that have already been trained. Proceed with the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<center>\n",
    "Continue with the <a href=\"31_machine_learning.ipynb\">next notebook</a> in a new browser tab.<br/><br/>\n",
    "<img src=\"images/footer.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
