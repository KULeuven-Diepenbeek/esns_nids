{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading network frames as if you were a hardware component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encapsulation of different networking protocols\n",
    "\n",
    "On any network messages are sent as frames. These frames travel over a physical medium (e.g. copper wire, EM radiation (WiFi), ...). When they arrive at a network host, they have to be interpreted and passed on the next component in the chain.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/01_network.png\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "From a **Networking 101** course (or by common sense ) you should know that files or other datasets are fragmented in a set of frames that are sent one-by-one over the chosen medium. In order to manage this massive amount data that is being sent, different networking protocols are used.\n",
    "\n",
    "These different network protocols typically encapsulate each other. All of these *layers* work in a similar fashion: first they provide meta-data in a **header**; next follows the **payload**. The payload of one layer contains the entire part of the next layer, which again consists of a header and a payload. This concept continues similar to [Rusian Matryoshka dolls](https://en.wikipedia.org/wiki/Matryoshka_doll).\n",
    "\n",
    "<center>\n",
    "<img src=\"images/01_encaps.png\"/>\n",
    "</center>\n",
    "\n",
    "The image above shows a simplified version of the **encapsulation** of the different layers. One important remark to make is that one of the fields in a header sections defines which protocol is used in its payload. This we will use heavily :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "As we are not the first people on the globe that are making hardware for sending and receiving network frames, a lot of components already exists to *translate* the symbols on the physical layer to digital signals. The incoming data is often presented through some bus, which can typically be 8, 32 or even 64 bits. This can be even higher when you're looking at networking hardware in server rooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "For the purposes of this workshop, we will work with a custom subset of the [CIC-IDS2017](https://www.unb.ca/cic/datasets/ids-2017.html). This is is a dataset that is commonly used in the network intrusion detection community, especially in the literature that performs intrusion detection using machine learning.\n",
    "\n",
    "This dataset roughly contains three kinds of traffic: Unlabelled traffic, labelled benign traffic and labelled attack traffic. Its authors created it by combining benign agents that generated regular, benign traffic with specific attacks. Spreading it over 5 weekdays, from Monday to Friday, each day contains a number of attacks.\n",
    "\n",
    "There are two types of data to encompass all this: Packet CAPture (.pcap) files that contain the binary packet traces as transmitted on the network, as well as Comma Seperated Value (.csv) files that list specific traffic flows alongside their specific features as well as their individual label.\n",
    "For this workshop, we used a subset of the data from Friday. Throughout the exercises you will discover what kinds of traffic are present is our selection.\n",
    "\n",
    "## Interface with the dataset\n",
    "\n",
    "The data we will use for this workshop is stored inside Numpy files (with extension .npy). To allow for easy use, we prepared an interface that provides access to packets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.dataset import NIDSDataset\n",
    "\n",
    "data_file = 'data/dataset_packets_v2.npy'\n",
    "labels_file = 'data/dataset_labels_v1.npy'\n",
    "\n",
    "dataset = NIDSDataset(data_file, labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading packets from the dataset simply requires iterating over the dataset in a for-loop, as is shown below.\n",
    "\n",
    "Please note that the previous **code-block** should have been executed prior to the for-loop example. The variable *dataset* is undefined if otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 1314 packets: True\n"
     ]
    }
   ],
   "source": [
    "packet_counter = 0\n",
    "\n",
    "for packet in dataset:\n",
    "    packet_counter += 1\n",
    "\n",
    "print(\"Dataset contains {} packets: {}\".format(packet_counter, packet_counter == len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this workshop, we will use data as sequentially presented in words of 32-bits, organised as an array of 4 bytes. This is achieved by iterating over a packet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_packet in dataset:\n",
    "    for word in example_packet:\n",
    "        print(word)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the packets are stored inside a Numpy file, each word is a Numpy-array, which should be considered when manipulating the data. For the sake of manageability, the packets are truncated a 100 bytes: Only the first 100 bytes of packets are stored inside the dataset. Shorter packets of course will contain fewer than 100 bytes, with the missing trailing bytes zero-padded in the Numpy-file. The zero-padding will not show up when iterating over a packet, as this will only provide actual packet bytes. Example: A packet containing 62 bytes will return a Numpy-array with 2 elements as its final word.\n",
    "\n",
    "Each packet has a label: Use the *get_label()* method to get a string that provides the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_labels = []\n",
    "\n",
    "for packet in dataset:\n",
    "    label = packet.get_label()\n",
    "    \n",
    "    if not (label in traffic_labels):\n",
    "        traffic_labels.append(label) \n",
    "\n",
    "print(\"These are the traffic classes present in the dataset: {}.\".format(\", \".join(traffic_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the dataset\n",
    "\n",
    "In this workshop data will seen in the same way it is seen by hardware: an incoming stream of words in which we (typically) cannot **'rewind'**. Once the word has passed, it won't come around again, and hence we are working on **line speed**.\n",
    "\n",
    "If we assume a 125 MHz clock and we assume that every clock cycle one 32-bit words is available, this provides 32 bits per 8 ns. This comes down to throughput of 4 bits per ns, or 4 gbps.\n",
    "\n",
    "\n",
    "To parse the incoming data you do need to know which bytes in the network stream contain which data. A more thorough explanation on each of these bytes can be found on wikipedia for [Ethernet](https://en.wikipedia.org/wiki/Ethernet_frame#Ethernet_II), [IPv4](https://en.wikipedia.org/wiki/IPv4#Header), and [TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol#TCP_segment_structure). As a quick cheat sheet, the image below is given.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/11_outline.png\"/>\n",
    "</center>\n",
    "\n",
    "The image above should be interpreted with the X-axis as time, and the Y-axis as the 4 bytes that are presented. For example the 6th word (index 5) will contain the 16-bit IP checksum in words 0 and 1.\n",
    "\n",
    "Given this image and with the support of some counters it should be possible to parse all the data.\n",
    "\n",
    "## Now, it's up to you\n",
    "\n",
    "You can decide on which part of the workshop you work first:\n",
    "\n",
    "* [PART 1](10_regexes.ipynb): Parsing network frames to match a modest set of regexes;\n",
    "* [PART 2](20_counting.ipynb): Parsing network frames to extract *flow IDs* which are matched to a *maximum volume*;\n",
    "* [PART 3](30_machine_learning.ipynb): Parsing network frames to feed them to <s>the sharks</s> a trained Neural Network.\n",
    "\n",
    "\n",
    "Each part exists in its own Notebook for which the links are provided above. Clicking on one of these links will open up the chosen notebooks in a new tab. You can revisit *this* page to help you with parsing.\n",
    "\n",
    "<center>\n",
    "<br/>\n",
    "<b>We wish you a lot of fun. Enjoy !!</b>\n",
    "<br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<br/>\n",
    "<img src=\"images/footer.png\"/>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
